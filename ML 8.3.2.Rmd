---
title: "8.3.2"
author: "Group 14"
output:
  pdf_document: default
  html_document: default
---
# Lab 8.3.2: Fitting Regression Trees

## Fit the Tree

We fit a regression tree to the Boston Housing Data, which is available through the MASS package in R studio and include 14 variables and 506 observations.
First, we split data half and half into training and test sets , then fit the regression tree model on the training data only.

```{r}
library(tree)
library(MASS)
set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston = tree(medv~., Boston, subset = train)
summary(tree.boston)
```

## Plot Tree 

Notice that although the tree grown to full depth has 7 leaves, only four of the variables (rm, lstat, crim and age) have been used to construct this tree. In the context of a regression tree, the deviance is simply the sum of squared errors for the tree. Now, we plot the tree.

```{r}
plot(tree.boston)
text(tree.boston,pretty=0)
```

The rm variable measures the average number of rooms per dwelling, and the lstat variable measures the percentage of individuals with lower socioeconomic status. The tree shows that higher values of rm correspond to higher house values and lower values of rm correspond to cheap houses.

## Preform Cross-Validation 

Now we use 10-fold cross validation by using cv.tree() function in order to determine the optimal level of tree complexity. This will help us decide whether pruning the tree will improve performance.

```{r}
cv.boston=cv.tree(tree.boston)
cv.boston
```

## Plot the Cross-Validation 

```{r}
plot(cv.boston$size,cv.boston$dev,type='b')
```

Through the plot, we can find that the most complex tree is selected by cross-validation. However if we wanted to prune the tree, we would use the prune.tree() function as the following.

## Make model by using Best Pruned Tree 

```{r}
prune.boston=prune.tree(tree.boston,best=5)
summary(prune.boston)
plot(prune.boston)
text(prune.boston,pretty=0)
```

## Find the Test Error Rate 

We use the unpruned tree to make predictions on the test set to keep the cross-validation results.
```{r}
yhat = predict(tree.boston,newdata=Boston[-train,])
boston.test = Boston[-train,"medv"]
plot(yhat, boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
sqrt(mean((yhat-boston.test)^2))
```
We got the result that the test set MSE for the regression tree is 35.29, and the square root is around 5.940 which means that this model gives predictions that are within around $5,940 of the true median home value.





